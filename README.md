# Anthropicì˜ MCPë¡œ Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°í•˜ê¸°

[![Bright Data Promo](https://github.com/luminati-io/LinkedIn-Scraper/raw/main/Proxies%20and%20scrapers%20GitHub%20bonus%20banner.png)](https://brightdata.co.kr/)

ì´ ê°€ì´ë“œëŠ” ì˜¨ë””ë§¨ë“œ ë°ì´í„° ì¶”ì¶œì„ ìœ„í•œ MCP ì„œë²„ë¥¼ ì„¤ì •í•˜ê³ , ê°œë°œ ë„êµ¬ì™€ ì—°ê²°í•˜ë©°, Bright Dataë¥¼ í™œìš©í•´ AI í˜¸í™˜ ì›¹ ì •ë³´ë¥¼ ì¦‰ì‹œ í™•ë³´í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

- [ì œì•½ ì´í•´í•˜ê¸°: LLMì´ ì‹¤ì œ ì„¸ê³„ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë° ì™œ ë„ì›€ì´ í•„ìš”í•œê°€](#understanding-the-limitation-why-llms-need-help-with-real-world-interaction)
- [MCPì˜ ì¤‘ìš”ì„±](#the-importance-of-mcp)
- [Model Context Protocol ì´í•´í•˜ê¸°](#understanding-model-context-protocol)
- [MCP ì•„í‚¤í…ì²˜ ì„¤ëª…](#mcp-architecture-explained)
- [ë‚˜ë§Œì˜ MCP ì„œë²„ ê°œë°œí•˜ê¸°](#developing-your-own-mcp-server)
- [MCP ì„œë²„ ì—°ê²°í•˜ê¸°](#connecting-your-mcp-server)
- [ì „ë¬¸ì ì¸ ì›¹ ë°ì´í„° ì¶”ì¶œì„ ìœ„í•œ Bright Dataì˜ MCP ì‚¬ìš©í•˜ê¸°](#using-bright-datas-mcp-for-professional-web-data-extraction)
- [ì¶”ê°€ ì½ì„ê±°ë¦¬](#further-reading)

## Understanding the Limitation: Why LLMs Need Help with Real-World Interaction

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ ë°©ëŒ€í•œ í•™ìŠµ ë°ì´í„°ì…‹ì„ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  ìƒì„±í•˜ëŠ” ë° ë›°ì–´ë‚©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¤‘ìš”í•œ ì œì•½ì´ ìˆìŠµë‹ˆë‹¤. ì¦‰, ì™¸ë¶€ ì„¸ê³„ì™€ ìì—°ìŠ¤ëŸ½ê²Œ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” ë¡œì»¬ íŒŒì¼ì— ì ‘ê·¼í•˜ê±°ë‚˜, ì»¤ìŠ¤í…€ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ê±°ë‚˜, ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê¸°ëŠ¥ì´ ê¸°ë³¸ì ìœ¼ë¡œ ë‚´ì¥ë˜ì–´ ìˆì§€ ì•Šë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

ê¸°ë³¸ì ì¸ ì˜ˆë¥¼ ë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤. Claudeì—ê²Œ í™œì„± Amazon ìƒí’ˆ í˜ì´ì§€ì—ì„œ ì„¸ë¶€ ì •ë³´ë¥¼ ì¶”ì¶œí•´ ë‹¬ë¼ê³  ìš”ì²­í•˜ëŠ” ê²ƒì€ ì¶”ê°€ ë„êµ¬ ì—†ì´ëŠ” ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ì™œì¼ê¹Œìš”? ì¸í„°ë„·ì„ íƒìƒ‰í•˜ê±°ë‚˜ ì™¸ë¶€ ë™ì‘ì„ íŠ¸ë¦¬ê±°í•  ê³ ìœ í•œ ê¸°ëŠ¥ì´ ì—†ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

![claude-without-mcp](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/claude-without-mcp.png)

ë³´ì¡° ë„êµ¬ ì—†ì´ LLMì€ ì‹¤ì‹œê°„ ë°ì´í„°ì— ì˜ì¡´í•˜ê±°ë‚˜ ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ì˜ í†µí•©ì´ í•„ìš”í•œ ì‹¤ë¬´ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ë°”ë¡œ ì—¬ê¸°ì—ì„œ [Anthropic's Model Context Protocol (MCP)](https://www.anthropic.com/news/model-context-protocol)ê°€ ìœ ìš©í•©ë‹ˆë‹¤. MCPëŠ” LLMì´ ë°ì´í„° ì¶”ì¶œê¸°, API, ìŠ¤í¬ë¦½íŠ¸ ê°™ì€ ì™¸ë¶€ ë„êµ¬ì™€ ì•ˆì „í•˜ê³  í‘œì¤€í™”ëœ ë°©ì‹ìœ¼ë¡œ í†µì‹ í•  ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤.

ë‹¤ìŒì€ ì‹¤ì œë¡œ ë‹¬ë¼ì§€ëŠ” ì ì…ë‹ˆë‹¤. ì»¤ìŠ¤í…€ MCP ì„œë²„ë¥¼ í†µí•©í•œ ë’¤, Claudeë¥¼ í†µí•´ êµ¬ì¡°í™”ëœ Amazon ìƒí’ˆ ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì§ì ‘ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.

![claude-amazon-product-data-extraction-results](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/claude-amazon-product-data-extraction-results.png)

## The Importance of MCP

- **í‘œì¤€í™”:** MCPëŠ” LLM ê¸°ë°˜ ì‹œìŠ¤í…œì´ ì™¸ë¶€ ë„êµ¬ ë° ë°ì´í„°ì— ì—°ê²°í•  ìˆ˜ ìˆëŠ” í†µì¼ëœ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” APIê°€ ì›¹ í†µí•©ì„ í‘œì¤€í™”í•œ ë°©ì‹ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì»¤ìŠ¤í…€ í†µí•©ì˜ í•„ìš”ì„±ì´ í¬ê²Œ ì¤„ì–´ ê°œë°œì´ ê°€ì†í™”ë©ë‹ˆë‹¤.
- **ìœ ì—°ì„±ê³¼ í™•ì¥ì„±:** ê°œë°œìëŠ” ë„êµ¬ í†µí•©ì„ ë‹¤ì‹œ ì‘ì„±í•˜ì§€ ì•Šê³ ë„ LLM ë˜ëŠ” í˜¸ìŠ¤íŒ… í”Œë«í¼ì„ êµì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. MCPëŠ” `stdio` ê°™ì€ ì—¬ëŸ¬ í†µì‹  ë°©ì‹ì„ ì§€ì›í•˜ë¯€ë¡œ ë‹¤ì–‘í•œ êµ¬ì„±ì— ì ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **LLM ê¸°ëŠ¥ ê°•í™”:** MCPëŠ” LLMì„ ìµœì‹  ë°ì´í„° ë° ì™¸ë¶€ ë„êµ¬ì™€ ì—°ê²°í•¨ìœ¼ë¡œì¨ ì •ì ì¸ ì‘ë‹µì„ ë„˜ì–´ì„¤ ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. ì´ì œ LLMì€ ìµœì‹ ì˜ ê´€ë ¨ ì •ë³´ë¥¼ ì œê³µí•˜ê³ , ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¼ ì‹¤ì œ ì„¸ê³„ì˜ ë™ì‘ì„ íŠ¸ë¦¬ê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **ë¹„ìœ **:
> 
> MCPëŠ” LLMì„ ìœ„í•œ USB ì¸í„°í˜ì´ìŠ¤ë¼ê³  ìƒê°í•˜ì‹œë©´ ë©ë‹ˆë‹¤. USBê°€ íŠ¹ë³„í•œ ë“œë¼ì´ë²„ ì—†ì´ë„ ë‹¤ì–‘í•œ ì¥ì¹˜(í‚¤ë³´ë“œ, í”„ë¦°í„°, ì™¸ì¥ ë“œë¼ì´ë¸Œ)ë¥¼ í˜¸í™˜ ê¸°ê¸°ì— ì—°ê²°í•  ìˆ˜ ìˆê²Œ í•´ì£¼ë“¯, MCPëŠ” í‘œì¤€í™”ëœ í”„ë¡œí† ì½œì„ í†µí•´ LLMì´ í­ë„“ì€ ë„êµ¬ì— ì—°ê²°ë˜ë„ë¡ í•´ì¤ë‹ˆë‹¤. ë§¤ë²ˆ ì»¤ìŠ¤í…€ í†µí•©ì„ í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.

## Understanding Model Context Protocol

Model Context Protocol(MCP)ì€ Anthropicì´ ê°œë°œí•œ ì˜¤í”ˆ í‘œì¤€ìœ¼ë¡œ, ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì´ ì™¸ë¶€ ë„êµ¬, API, ë°ì´í„° ì†ŒìŠ¤ì™€ ì¼ê´€ë˜ê³  ì•ˆì „í•œ ë°©ì‹ìœ¼ë¡œ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. MCPëŠ” ë²”ìš© ì»¤ë„¥í„°ë¡œ ê¸°ëŠ¥í•˜ë©°, LLMì´ ì›¹ì‚¬ì´íŠ¸ ë°ì´í„° ì¶”ì¶œ, ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬, ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ê³¼ ê°™ì€ í˜„ì‹¤ ì„¸ê³„ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.

Anthropicì´ ì´ë¥¼ ì†Œê°œí–ˆì§€ë§Œ MCPëŠ” ê°œë°©í˜•ì´ë©° í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì¦‰, ëˆ„êµ¬ë‚˜ í‘œì¤€ì„ êµ¬í˜„í•˜ê±°ë‚˜ ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [Retrieval-Augmented Generation (RAG)](https://brightdata.co.kr/blog/web-data/rag-explained)ë¥¼ ì‚¬ìš©í•´ ë³¸ ì ì´ ìˆë‹¤ë©´ ì´ ê°œë…ì„ ì´í•´í•˜ê¸° ì‰¬ìš°ì‹¤ ê²ƒì…ë‹ˆë‹¤. MCPëŠ” ê²½ëŸ‰ JSON-RPC ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ìƒí˜¸ì‘ìš©ì„ í‘œì¤€í™”í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì´ ë¼ì´ë¸Œ ë°ì´í„°ì— ì ‘ê·¼í•˜ê³  ë™ì‘ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤ëŠ” ì ì—ì„œ ê·¸ ì•„ì´ë””ì–´ë¥¼ í™•ì¥í•©ë‹ˆë‹¤.

## MCP Architecture Explained

ê¸°ë³¸ì ìœ¼ë¡œ MCPëŠ” AI ëª¨ë¸ê³¼ ì™¸ë¶€ ê¸°ëŠ¥ ê°„ì˜ í†µì‹ ì„ í‘œì¤€í™”í•©ë‹ˆë‹¤.

**í•µì‹¬ ì•„ì´ë””ì–´:** í‘œì¤€í™”ëœ ì¸í„°í˜ì´ìŠ¤(ì¼ë°˜ì ìœ¼ë¡œ `stdio` ê°™ì€ ì „ì†¡ ê³„ì¸µ ìœ„ì—ì„œ JSON-RPC 2.0 ì‚¬ìš©)ë¥¼ í†µí•´ LLM(í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´)ì´ ì™¸ë¶€ ì„œë²„ê°€ ë…¸ì¶œí•˜ëŠ” ë„êµ¬ë¥¼ íƒìƒ‰í•˜ê³  í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

MCPëŠ” ì„¸ ê°€ì§€ í•µì‹¬ êµ¬ì„± ìš”ì†Œë¥¼ ê°–ì¶˜ í´ë¼ì´ì–¸íŠ¸-ì„œë²„ ì•„í‚¤í…ì²˜ë¡œ ë™ì‘í•©ë‹ˆë‹¤.

1. **MCP Host**: LLMê³¼ ì™¸ë¶€ ë„êµ¬ ê°„ ìƒí˜¸ì‘ìš©ì„ ì‹œì‘í•˜ê³  ê´€ë¦¬í•˜ëŠ” í™˜ê²½ ë˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. ì˜ˆë¡œëŠ” _Claude Desktop_ ê°™ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ë‚˜ _Cursor_ ê°™ì€ IDEê°€ ìˆìŠµë‹ˆë‹¤.
2. **MCP Client**: í˜¸ìŠ¤íŠ¸ ë‚´ë¶€ì˜ êµ¬ì„± ìš”ì†Œë¡œ, MCP Serverì™€ì˜ ì—°ê²°ì„ ì„¤ì •í•˜ê³  ìœ ì§€í•˜ë©° í†µì‹  í”„ë¡œí† ì½œì„ ì²˜ë¦¬í•˜ê³  ë°ì´í„° êµí™˜ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
3. **MCP Server:** (ê°œë°œìê°€ ë§Œë“œëŠ”) MCP í”„ë¡œí† ì½œì„ êµ¬í˜„í•˜ê³  íŠ¹ì • ê¸°ëŠ¥ ì§‘í•©ì„ ë…¸ì¶œí•˜ëŠ” í”„ë¡œê·¸ë¨ì…ë‹ˆë‹¤. MCP ì„œë²„ëŠ” ë°ì´í„°ë² ì´ìŠ¤, ì›¹ ì„œë¹„ìŠ¤ ë˜ëŠ” (ì´ ê¸€ì˜ ê²½ìš°) ì›¹ì‚¬ì´íŠ¸(Amazon)ì™€ ì¸í„°í˜ì´ìŠ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„œë²„ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í‘œì¤€í™”ëœ ë°©ì‹ìœ¼ë¡œ ê¸°ëŠ¥ì„ ë…¸ì¶œí•©ë‹ˆë‹¤:
   - **Tools:** í˜¸ì¶œ ê°€ëŠ¥í•œ í•¨ìˆ˜(ì˜ˆ: _scrape\_amazon\_product_, _get\_weather\_data_)
   - **Resources:** ì •ì  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ì½ê¸° ì „ìš© ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ(ì˜ˆ: íŒŒì¼ì„ ê°€ì ¸ì˜¤ê¸°, JSON ë ˆì½”ë“œ ë°˜í™˜)
   - **Prompts:** ë„êµ¬ ë° ë¦¬ì†ŒìŠ¤ì™€ì˜ LLM ìƒí˜¸ì‘ìš©ì„ ì•ˆë‚´í•˜ëŠ” ì‚¬ì „ ì •ì˜ í…œí”Œë¦¿

ë‹¤ìŒì€ MCP ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨ì…ë‹ˆë‹¤:

![mcp-architecture-diagram-host-client-server-connections](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/mcp-architecture-diagram-host-client-server-connections.png)

_Image Source: [Model Context Protocol](https://modelcontextprotocol.io/introduction)_

ì´ êµ¬ì„±ì—ì„œ **host**(Claude Desktop ë˜ëŠ” Cursor IDE)ê°€ **MCP client**ë¥¼ ì‹¤í–‰í•˜ê³ , ê·¸ í´ë¼ì´ì–¸íŠ¸ê°€ ì™¸ë¶€ **MCP server**ì— ì—°ê²°í•©ë‹ˆë‹¤. í•´ë‹¹ ì„œë²„ëŠ” tools, resources, promptsë¥¼ ë…¸ì¶œí•˜ì—¬ AIê°€ í•„ìš”í•  ë•Œ ì´ë¥¼ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

ìš”ì•½í•˜ë©´ ì›Œí¬í”Œë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë™ì‘í•©ë‹ˆë‹¤:

- ì‚¬ìš©ìê°€ _"ì´ Amazon ë§í¬ì—ì„œ ìƒí’ˆ ì •ë³´ë¥¼ ê°€ì ¸ì™€."_ ê°™ì€ ë©”ì‹œì§€ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
- MCP clientê°€ í•´ë‹¹ ì‘ì—…ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë“±ë¡ëœ toolì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
- í´ë¼ì´ì–¸íŠ¸ê°€ MCP serverë¡œ êµ¬ì¡°í™”ëœ ë¦¬ã‚¯ã‚¨ã‚¹ãƒˆë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.
- MCP serverê°€ ì ì ˆí•œ ë™ì‘ì„ ì‹¤í–‰í•©ë‹ˆë‹¤(ì˜ˆ: í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € ì‹¤í–‰).
- ì„œë²„ê°€ êµ¬ì¡°í™”ëœ ê²°ê³¼ë¥¼ MCP clientë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
- í´ë¼ì´ì–¸íŠ¸ê°€ ê²°ê³¼ë¥¼ LLMë¡œ ì „ë‹¬í•˜ê³ , LLMì´ ì´ë¥¼ ì‚¬ìš©ìì—ê²Œ ì œì‹œí•©ë‹ˆë‹¤.

## Developing Your Own MCP Server

Amazon ìƒí’ˆ í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” Python MCP serverë¥¼ êµ¬ì¶•í•´ ë³´ê² ìŠµë‹ˆë‹¤.

![amazon-product-page-example](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/amazon-product-page-example.png)

ì´ ì„œë²„ëŠ” ë‘ ê°€ì§€ toolì„ ì œê³µí•©ë‹ˆë‹¤. í•˜ë‚˜ëŠ” HTMLì„ ë‹¤ìš´ë¡œë“œí•˜ê³ , ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì •ë¦¬ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. Cursor ë˜ëŠ” Claude Desktopì˜ LLM clientë¥¼ í†µí•´ ì„œë²„ì™€ ìƒí˜¸ì‘ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.

### Step 1: Preparing Your Environment

ë¨¼ì € [Python 3](https://www.python.org/downloads/)ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ê°€ìƒ í™˜ê²½ì„ ìƒì„±í•˜ê³  í™œì„±í™”í•©ë‹ˆë‹¤:

```sh
python -m venv mcp-amazon-scraper
# On macOS/Linux:
source mcp-amazon-scraper/bin/activate
# On Windows:
.\mcp-amazon-scraper\Scripts\activate
```

í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk), [Playwright](https://playwright.dev/python/), [LXML](https://lxml.de/)ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤.

```sh
pip install mcp playwright lxml
# Install browser binaries for Playwright
python -m playwright install
```

ì´ ëª…ë ¹ì€ ë‹¤ìŒì„ ì„¤ì¹˜í•©ë‹ˆë‹¤:

- **mcp**: ëª¨ë“  JSON-RPC í†µì‹  ì„¸ë¶€ ì‚¬í•­ì„ ì²˜ë¦¬í•˜ëŠ” Model Context Protocol ì„œë²„/í´ë¼ì´ì–¸íŠ¸ë¥¼ ìœ„í•œ Python SDKì…ë‹ˆë‹¤.
- **playwright**: JavaScript ë¹„ì¤‘ì´ í° ì›¹ì‚¬ì´íŠ¸ë¥¼ ë Œë”ë§í•˜ê³  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°í•˜ê¸° ìœ„í•œ í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ë¸Œë¼ìš°ì € ìë™í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
- **lxml**: XPath ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•´ ì›¹ í˜ì´ì§€ì—ì„œ íŠ¹ì • ë°ì´í„° ìš”ì†Œë¥¼ ì‰½ê²Œ ì¶”ì¶œí•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¹ ë¥¸ XML/HTML íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

ìš”ì•½í•˜ë©´, MCP Python SDK(`mcp`)ê°€ í”„ë¡œí† ì½œ ì„¸ë¶€ ì‚¬í•­ì„ ëª¨ë‘ ì²˜ë¦¬í•´ ì£¼ë¯€ë¡œ Claude ë˜ëŠ” Cursorê°€ ìì—°ì–´ í”„ë¡¬í”„íŠ¸ë¡œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” toolì„ ë…¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. PlaywrightëŠ” ì›¹ í˜ì´ì§€(åŒ…æ‹¬ JavaScript ì½˜í…ì¸ )ë¥¼ ì™„ì „íˆ ë Œë”ë§í•  ìˆ˜ ìˆê²Œ í•´ì£¼ê³ , lxmlì€ ê°•ë ¥í•œ HTML íŒŒì‹± ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

### Step 2: Starting the MCP Server

`amazon_scraper_mcp.py`ë¼ëŠ” Python íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤. ë¨¼ì € í•„ìš”í•œ ëª¨ë“ˆì„ ì„í¬íŠ¸í•˜ê³  `FastMCP` serverë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤:

```python
import os
import asyncio
from lxml import html as lxml_html
from mcp.server.fastmcp import FastMCP
from playwright.async_api import async_playwright

# Define a temporary file path for the HTML content
HTML_FILE = os.path.join(os.getenv("TMPDIR", "/tmp"), "amazon_product_page.html")

# Initialize the MCP server with a descriptive name
mcp = FastMCP("Amazon Product Scraper")

print("MCP Server Initialized: Amazon Product Scraper")
```

ì´ ì½”ë“œëŠ” MCP server ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ì œ ì—¬ê¸°ì— toolì„ ì¶”ê°€í•˜ê² ìŠµë‹ˆë‹¤.

### Step 3: Implementing the `fetch_page` Tool

ì´ toolì€ URLì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ Playwrightë¡œ í˜ì´ì§€ì— ì´ë™í•˜ê³ , ì½˜í…ì¸ ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦° ë’¤ HTMLì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì„ì‹œ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.

```python
@mcp.tool()
async def fetch_page(url: str) -> str:
    """
    Fetches the HTML content of the given Amazon product URL using Playwright
    and saves it to a temporary file. Returns a status message.
    """
    print(f"Executing fetch_page for URL: {url}")
    try:
        async with async_playwright() as p:
            # Launch headless Chromium browser
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            # Navigate to the URL with a generous timeout
            await page.goto(url, timeout=90000, wait_until="domcontentloaded")
            # Wait for a key element (e.g., body) to ensure basic loading
            await page.wait_for_selector("body", timeout=30000)
            # Add a small delay for any dynamic content rendering via JavaScript
            await asyncio.sleep(5)

            html_content = await page.content()
            with open(HTML_FILE, "w", encoding="utf-8") as f:
                f.write(html_content)

            await browser.close()
            print(f"Successfully fetched and saved HTML to {HTML_FILE}")
            return f"HTML content for {url} downloaded and saved successfully to {HTML_FILE}."
    except Exception as e:
        error_message = f"Error fetching page {url}: {str(e)}"
        print(error_message)
        return error_message
```

ì´ ë¹„ë™ê¸° í•¨ìˆ˜ëŠ” Amazon í˜ì´ì§€ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” JavaScript ë Œë”ë§ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ Playwrightë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. `@mcp.tool()` ë°ì½”ë ˆì´í„°ëŠ” ì´ í•¨ìˆ˜ë¥¼ server ë‚´ì—ì„œ í˜¸ì¶œ ê°€ëŠ¥í•œ toolë¡œ ë“±ë¡í•©ë‹ˆë‹¤.

### Step 4: Implementing the `extract_info` Tool

ì´ toolì€ `fetch_page`ê°€ ì €ì¥í•œ HTML íŒŒì¼ì„ ì½ê³ , LXMLê³¼ XPath ì…€ë ‰í„°ë¡œ íŒŒì‹±í•œ ë’¤, ì¶”ì¶œí•œ ìƒí’ˆ ì„¸ë¶€ ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
def _extract_xpath(tree, xpath, default="N/A"):
    """Helper function to extract text using XPath, returning default if not found."""
    try:
        # Use text_content() to get text from node and children, strip whitespace
        result = tree.xpath(xpath)
        if result:
            return result[0].text_content().strip()
        return default
    except Exception:
        return default

def _extract_price(price_str):
    """Helper function to parse price string into a float."""
    if price_str == "N/A":
        return None
    try:
        # Remove currency symbols and commas, handle potential whitespace
        cleaned_price = "".join(filter(str.isdigit or str.__eq__("."), price_str))
        return float(cleaned_price)
    except (ValueError, TypeError):
        return None

@mcp.tool()
def extract_info() -> dict:
    """
    Parses the saved HTML file (downloaded by fetch_page) to extract
    Amazon product details like title, price, rating, features, etc.
    Returns a dictionary of the extracted data.
    """
    print(f"Executing extract_info from file: {HTML_FILE}")
    if not os.path.exists(HTML_FILE):
        return {
            "error": f"HTML file not found at {HTML_FILE}. Please run fetch_page first."
        }

    try:
        with open(HTML_FILE, "r", encoding="utf-8") as f:
            page_html = f.read()

        tree = lxml_html.fromstring(page_html)

        # --- XPath Selectors for Amazon Product Details ---
        title = _extract_xpath(tree, '//span[@id="productTitle"]')
        # Handle different price structures (main price, sale price)
        price_whole = _extract_xpath(tree, '//span[contains(@class, "a-price-whole")]')
        price_fraction = _extract_xpath(
            tree, '//span[contains(@class, "a-price-fraction")]'
        )
        price_str = (
            f"{price_whole}.{price_fraction}"
            if price_whole != "N/A"
            else _extract_xpath(tree, '//span[contains(@class,"a-offscreen")]')
        )  # Fallback to offscreen if needed

        price = _extract_price(price_str)

        # Original price (strike-through)
        original_price_str = _extract_xpath(
            tree, '//span[@class="a-price a-text-price"]//span[@class="a-offscreen"]'
        )
        original_price = _extract_price(original_price_str)

        # Rating
        rating_text = _extract_xpath(tree, '//span[@id="acrPopover"]/@title')
        rating = None
        if rating_text != "N/A":
            try:
                rating = float(rating_text.split()[0])
            except (ValueError, IndexError):
                rating = None

        # Review Count
        reviews_text = _extract_xpath(tree, '//span[@id="acrCustomerReviewText"]')
        review_count = None
        if reviews_text != "N/A":
            try:
                review_count = int(reviews_text.split()[0].replace(",", ""))
            except (ValueError, IndexError):
                review_count = None

        # Availability
        availability = _extract_xpath(
            tree,
            '//div[@id="availability"]//span/text()',
        )

        # Features (bullet points)
        feature_elements = tree.xpath(
            '//div[@id="feature-bullets"]//li//span[@class="a-list-item"]'
        )
        features = [
            elem.text_content().strip()
            for elem in feature_elements
            if elem.text_content().strip()
        ]

        # Calculate Discount
        discount = None
        if price and original_price and original_price > price:
            discount = round(((original_price - price) / original_price) * 100)

        extracted_data = {
            "title": title,
            "price": price,
            "original_price": original_price,
            "discount_percent": discount,
            "rating_stars": rating,
            "review_count": review_count,
            "features": features,
            "availability": availability.strip(),
        }
        print(f"Successfully extracted data: {extracted_data}")
        return extracted_data

    except Exception as e:
        error_message = f"Error parsing HTML: {str(e)}"
        print(error_message)  # Added for logging
        return {"error": error_message}
```

ì´ í•¨ìˆ˜ëŠ” LXMLì˜ `fromstring`ìœ¼ë¡œ HTMLì„ íŒŒì‹±í•˜ê³ , ê²¬ê³ í•œ XPath ì…€ë ‰í„°ë¡œ ì›í•˜ëŠ” ìš”ì†Œë¥¼ ì°¾ìŠµë‹ˆë‹¤.

### Step 5: Running the Server

ë§ˆì§€ë§‰ìœ¼ë¡œ `amazon_scraper_mcp.py` ìŠ¤í¬ë¦½íŠ¸ ëì— ë‹¤ìŒ ì¤„ì„ ì¶”ê°€í•˜ì—¬ `stdio` ì „ì†¡ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ serverë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ì´ëŠ” Claude Desktop ë˜ëŠ” Cursor ê°™ì€ í´ë¼ì´ì–¸íŠ¸ì™€ í†µì‹ í•˜ëŠ” ë¡œì»¬ MCP serverì—ì„œ í‘œì¤€ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

```python
if __name__ == "__main__":
    print("Starting MCP Server with stdio transport...")
    # Run the server, listening via standard input/output
    mcp.run(transport="stdio")
```

### Complete Source Code

```python
import os
import asyncio
from lxml import html as lxml_html
from mcp.server.fastmcp import FastMCP
from playwright.async_api import async_playwright

# Define a temporary file path for the HTML content
HTML_FILE = os.path.join(os.getenv("TMPDIR", "/tmp"), "amazon_product_page.html")

# Initialize the MCP server with a descriptive name
mcp = FastMCP("Amazon Product Scraper")

print("MCP Server Initialized: Amazon Product Scraper")

@mcp.tool()
async def fetch_page(url: str) -> str:
    """
    Fetches the HTML content of the given Amazon product URL using Playwright
    and saves it to a temporary file. Returns a status message.
    """
    print(f"Executing fetch_page for URL: {url}")
    try:
        async with async_playwright() as p:
            # Launch headless Chromium browser
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            # Navigate to the URL with a generous timeout
            await page.goto(url, timeout=90000, wait_until="domcontentloaded")
            # Wait for a key element (e.g., body) to ensure basic loading
            await page.wait_for_selector("body", timeout=30000)
            # Add a small delay for any dynamic content rendering via JavaScript
            await asyncio.sleep(5)

            html_content = await page.content()
            with open(HTML_FILE, "w", encoding="utf-8") as f:
                f.write(html_content)

            await browser.close()
            print(f"Successfully fetched and saved HTML to {HTML_FILE}")
            return f"HTML content for {url} downloaded and saved successfully to {HTML_FILE}."
    except Exception as e:
        error_message = f"Error fetching page {url}: {str(e)}"
        print(error_message)
        return error_message

def _extract_xpath(tree, xpath, default="N/A"):
    """Helper function to extract text using XPath, returning default if not found."""
    try:
        # Use text_content() to get text from node and children, strip whitespace
        result = tree.xpath(xpath)
        if result:
            return result[0].text_content().strip()
        return default
    except Exception:
        return default

def _extract_price(price_str):
    """Helper function to parse price string into a float."""
    if price_str == "N/A":
        return None
    try:
        # Remove currency symbols and commas, handle potential whitespace
        cleaned_price = "".join(filter(str.isdigit or str.__eq__("."), price_str))
        return float(cleaned_price)
    except (ValueError, TypeError):
        return None

@mcp.tool()
def extract_info() -> dict:
    """
    Parses the saved HTML file (downloaded by fetch_page) to extract
    Amazon product details like title, price, rating, features, etc.
    Returns a dictionary of the extracted data.
    """
    print(f"Executing extract_info from file: {HTML_FILE}")
    if not os.path.exists(HTML_FILE):
        return {
            "error": f"HTML file not found at {HTML_FILE}. Please run fetch_page first."
        }

    try:
        with open(HTML_FILE, "r", encoding="utf-8") as f:
            page_html = f.read()

        tree = lxml_html.fromstring(page_html)

        # --- XPath Selectors for Amazon Product Details ---
        title = _extract_xpath(tree, '//span[@id="productTitle"]')
        # Handle different price structures (main price, sale price)
        price_whole = _extract_xpath(tree, '//span[contains(@class, "a-price-whole")]')
        price_fraction = _extract_xpath(
            tree, '//span[contains(@class, "a-price-fraction")]'
        )
        price_str = (
            f"{price_whole}.{price_fraction}"
            if price_whole != "N/A"
            else _extract_xpath(tree, '//span[contains(@class,"a-offscreen")]')
        )  # Fallback to offscreen if needed

        price = _extract_price(price_str)

        # Original price (strike-through)
        original_price_str = _extract_xpath(
            tree, '//span[@class="a-price a-text-price"]//span[@class="a-offscreen"]'
        )
        original_price = _extract_price(original_price_str)

        # Rating
        rating_text = _extract_xpath(tree, '//span[@id="acrPopover"]/@title')
        rating = None
        if rating_text != "N/A":
            try:
                rating = float(rating_text.split()[0])
            except (ValueError, IndexError):
                rating = None

        # Review Count
        reviews_text = _extract_xpath(tree, '//span[@id="acrCustomerReviewText"]')
        review_count = None
        if reviews_text != "N/A":
            try:
                review_count = int(reviews_text.split()[0].replace(",", ""))
            except (ValueError, IndexError):
                review_count = None

        # Availability
        availability = _extract_xpath(
            tree,
            '//div[@id="availability"]//span/text()',
        )

        # Features (bullet points)
        feature_elements = tree.xpath(
            '//div[@id="feature-bullets"]//li//span[@class="a-list-item"]'
        )
        features = [
            elem.text_content().strip()
            for elem in feature_elements
            if elem.text_content().strip()
        ]

        # Calculate Discount
        discount = None
        if price and original_price and original_price > price:
            discount = round(((original_price - price) / original_price) * 100)

        extracted_data = {
            "title": title,
            "price": price,
            "original_price": original_price,
            "discount_percent": discount,
            "rating_stars": rating,
            "review_count": review_count,
            "features": features,
            "availability": availability.strip(),
        }
        print(f"Successfully extracted data: {extracted_data}")
        return extracted_data

    except Exception as e:
        error_message = f"Error parsing HTML: {str(e)}"
        print(error_message)  # Added for logging
        return {"error": error_message}

if __name__ == "__main__":
    print("Starting MCP Server with stdio transport...")
    # Run the server, listening via standard input/output
    mcp.run(transport="stdio")
```

## Connecting Your MCP Server

ì´ì œ server scriptê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë‹ˆ, Claude Desktop ë° Cursor ê°™ì€ MCP clientì— ì—°ê²°í•´ ë³´ê² ìŠµë‹ˆë‹¤.

### Setting Up with Claude Desktop

**Step 1:** Claude Desktopì„ ì—½ë‹ˆë‹¤.

**Step 2:** `Settings` -> `Developer` -> `Edit Config`ë¡œ ì´ë™í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ê¸°ë³¸ í…ìŠ¤íŠ¸ í¸ì§‘ê¸°ì—ì„œ `claude_desktop_config.json` íŒŒì¼ì´ ì—´ë¦½ë‹ˆë‹¤.

![claude-desktop-settings-menu-navigation](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/claude-desktop-settings-menu-navigation.png)

**Step 3:** `mcpServers` í‚¤ ì•„ë˜ì— server í•­ëª©ì„ ì¶”ê°€í•©ë‹ˆë‹¤. `args`ì˜ ê²½ë¡œë¥¼ `amazon_scraper_mcp.py` íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œë¡œ ë°”ê¾¸ëŠ” ê²ƒì„ ìŠì§€ ë§ˆì‹­ì‹œì˜¤.

```json
{
  "mcpServers": {
    "amazon_product_scraper": {
      "command": "python",  // Or python3 if needed
      "args": ["/full/path/to/your/amazon_scraper_mcp.py"], // <-- IMPORTANT: Use the correct absolute path
    }
  }
}
```

**Step 4:** `claude_desktop_config.json` íŒŒì¼ì„ ì €ì¥í•œ ë’¤, ë³€ê²½ ì‚¬í•­ì´ ì ìš©ë˜ë„ë¡ Claude Desktopì„ ì™„ì „íˆ ì¢…ë£Œí–ˆë‹¤ê°€ ë‹¤ì‹œ ì‹¤í–‰í•©ë‹ˆë‹¤.

**Step 5:** ì´ì œ Claude Desktopì˜ ì±„íŒ… ì…ë ¥ ì˜ì—­ì— ì‘ì€ ë„êµ¬ ì•„ì´ì½˜(ë§ì¹˜ ğŸ”¨ ê°™ì€)ì´ í‘œì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

![claude-desktop-mcp-tools-icon-interface](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/claude-desktop-mcp-tools-icon-interface.png)

**Step 6:** ì´ë¥¼ í´ë¦­í•˜ë©´ `fetch_page` ë° `extract_info` toolsë¥¼ í¬í•¨í•œ "Amazon Product Scraper"ê°€ ë‚˜ì—´ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

![claude-available-mcp-tools-dialog-amazon-scraper](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/claude-available-mcp-tools-dialog-amazon-scraper.png)

**Step 7:** ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ëƒ…ë‹ˆë‹¤: _"Get the current price, original price, and rating for this Amazon product: [https://www.amazon.com/dp/B09C13PZX7](https://www.amazon.com/dp/B09C13PZX7)"._

**Step 8:** ClaudeëŠ” ì™¸ë¶€ ë„êµ¬ê°€ í•„ìš”í•¨ì„ ê°ì§€í•˜ê³  ë¨¼ì € `fetch_page`, ê·¸ ë‹¤ìŒ `extract_info`ë¥¼ ì‹¤í–‰í•  ê¶Œí•œì„ ìš”ì²­í•©ë‹ˆë‹¤. ê° toolì— ëŒ€í•´ "Allow for this chat"ë¥¼ í´ë¦­í•©ë‹ˆë‹¤.

![mcp-permission-dialog-fetch-page-amazon-tool](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/mcp-permission-dialog-fetch-page-amazon-tool.png)

**Step 9:** ê¶Œí•œì„ ë¶€ì—¬í•˜ë©´ MCP serverê°€ toolsë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤. ClaudeëŠ” êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•œ í›„ ì±„íŒ…ì—ì„œ ì´ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

![claude-amazon-product-data-extraction-results](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/claude-amazon-product-data-extraction-results-2.png)

### Setting Up with Cursor

Cursor(AI-first IDE)ì˜ ê³¼ì •ë„ ìœ ì‚¬í•©ë‹ˆë‹¤.

**Step 1:** Cursorë¥¼ ì—½ë‹ˆë‹¤.

**Step 2:** `Settings` âš™ï¸ë¡œ ì´ë™í•œ ë’¤ `MCP` ì„¹ì…˜ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.

![cursor-ide-add-new-global-mcp-server-settings](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/cursor-ide-add-new-global-mcp-server-settings.png)

**Step 3:** "+Add a new global MCP Server"ë¥¼ í´ë¦­í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë©´ `mcp.json` êµ¬ì„± íŒŒì¼ì´ ì—´ë¦½ë‹ˆë‹¤. server í•­ëª©ì„ ì¶”ê°€í•˜ë˜, ìŠ¤í¬ë¦½íŠ¸ì˜ **ì ˆëŒ€ ê²½ë¡œ**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

![cursor-mcp-json-configuration-file-amazon-scraper](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/cursor-mcp-json-configuration-file-amazon-scraper.png)

**Step 4:** `mcp.json` íŒŒì¼ì„ ì €ì¥í•˜ë©´ "amazon\_product\_scraper"ê°€ ëª©ë¡ì— í‘œì‹œë˜ê³ , ì‹¤í–‰ ë° ì—°ê²° ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë…¹ìƒ‰ ì ì´ í‘œì‹œë˜ê¸°ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![cursor-ide-configured-amazon-scraper-mcp-settings](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/cursor-ide-configured-amazon-scraper-mcp-settings.png)

**Step 5:** Cursorì˜ ì±„íŒ… ê¸°ëŠ¥(`Cmd+l` ë˜ëŠ” `Ctrl+l`)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

**Step 6:** ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ëƒ…ë‹ˆë‹¤: "_Extract all available product data from this Amazon URL: [https://www.amazon.com/dp/B09C13PZX7](https://www.amazon.com/dp/B09C13PZX7). Format the output as a structured JSON object"_.

**Step 7:** Claude Desktopê³¼ ë§ˆì°¬ê°€ì§€ë¡œ CursorëŠ” `fetch_page` ë° `extract_info` tools ì‹¤í–‰ ê¶Œí•œì„ ìš”ì²­í•©ë‹ˆë‹¤. ìš”ì²­ì„ ìŠ¹ì¸í•©ë‹ˆë‹¤("Run Tool").

**Step 8:** CursorëŠ” ìƒí˜¸ì‘ìš© íë¦„ì„ í‘œì‹œí•˜ë©°, MCP tool í˜¸ì¶œì„ ë³´ì—¬ì¤€ ë‹¤ìŒ `extract_info` toolì´ ë°˜í™˜í•œ êµ¬ì¡°í™”ëœ JSON ë°ì´í„°ë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì œì‹œí•©ë‹ˆë‹¤.

![cursor-ide-amazon-product-data-extraction-json-results](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/cursor-ide-amazon-product-data-extraction-json-results.png)
ë‹¤ìŒì€ Cursorì—ì„œì˜ JSON ì¶œë ¥ ì˜ˆì‹œì…ë‹ˆë‹¤:

```json
{
  "title": "Razer Basilisk V3 Customizable Ergonomic Gaming Mouse: Fastest Gaming Mouse Switch - Chroma RGB Lighting - 26K DPI Optical Sensor - 11 Programmable Buttons - HyperScroll Tilt Wheel - Classic Black",
  "price": 39.99,
  "original_price": 69.99,
  "discount_percent": 43,
  "rating_stars": 4.6,
  "review_count": 7782,
  "features": [
    "ICONIC ERGONOMIC DESIGN WITH THUMB REST â€” PC gaming mouse favored by millions worldwide with a form factor that perfectly supports the hand while its buttons are optimally positioned for quick and easy access",
    "11 PROGRAMMABLE BUTTONS â€” Assign macros and secondary functions across 11 programmable buttons to execute essential actions like push-to-talk, ping, and more",
    "HYPERSCROLL TILT WHEEL â€” Speed through content with a scroll wheel that free-spins until its stopped or switch to tactile mode for more precision and satisfying feedback that's ideal for cycling through weapons or skills",
    "11 RAZER CHROMA RGB LIGHTING ZONES â€” Customize each zone from over 16.8 million colors and countless lighting effects, all while it reacts dynamically with over 150 Chroma integrated games",
    "OPTICAL MOUSE SWITCHES GEN 2 â€” With zero unintended misclicks these switches provide crisp, responsive execution at a blistering 0.2ms actuation speed for up to 70 million clicks",
    "FOCUS+ 26K DPI OPTICAL SENSOR â€” Best-in-class mouse sensor with intelligent functions flawlessly tracks movement with zero smoothing, allowing for crisp response and pixel-precise accuracy",
    // ... (other features)
  ],
  "availability": "In Stock"
}
```

ì´ëŠ” MCPì˜ ìœ ì—°ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë™ì¼í•œ serverê°€ ì„œë¡œ ë‹¤ë¥¸ client ì• í”Œë¦¬ì¼€ì´ì…˜ê³¼ë„ ë§¤ë„ëŸ½ê²Œ ë™ì‘í•©ë‹ˆë‹¤.

## Using Bright Data's MCP for Professional Web Data Extraction

Bright Dataì˜ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ [Model Context Protocol (MCP)](https://github.com/luminati-io/brightdata-mcp) ì†”ë£¨ì…˜ì€ ìì²´ ê´€ë¦¬í˜• MCP serverì˜ ë³µì¡ì„±(ì˜ˆ: ãƒ—ãƒ­ã‚­ã‚· ê´€ë¦¬, [ã‚¢ãƒ³ãƒãƒœãƒƒãƒˆ ë‚´ë¹„ê²Œì´ì…˜](https://brightdata.co.kr/blog/web-data/anti-scraping-techniques), ìŠ¤ì¼€ì¼ë§ ê³¼ì œ)ì„ ì œê±°í•˜ê³ , [AI agents](https://brightdata.co.kr/use-cases/apps-agents) ë° LLMê³¼ì˜ ì›í™œí•œ í†µí•©ì„ ì œê³µí•©ë‹ˆë‹¤.

Bright Dataì˜ MCPì— ì—°ê²°í•˜ë©´ SERP ê²°ê³¼ ë° ì ‘ê·¼ì´ ì–´ë ¤ìš´ ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•œ ê³µê°œ ì›¹ ë°ì´í„°ì— ì¦‰ì‹œ ì ‘ê·¼í•  ìˆ˜ ìˆìœ¼ë©°, AI ì›Œí¬í”Œë¡œì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

MCPëŠ” [Web Unlocker](https://brightdata.co.kr/products/web-unlocker), [SERP API](https://brightdata.co.kr/products/serp-api), [Web Scraper API](https://brightdata.co.kr/products/web-scraper), [Scraping Browser](https://brightdata.co.kr/products/scraping-browser) ê°™ì€ ë„êµ¬ë¥¼ í†µí•´ ê°•ë ¥í•œ ì›¹ ì¶”ì¶œ í”„ë ˆì„ì›Œí¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

- **[AI-Ready Data](https://brightdata.co.kr/use-cases/data-for-ai):** ì‚¬ì „ êµ¬ì¡°í™”ëœ ì½˜í…ì¸ ë¡œ, ì „ì²˜ë¦¬ê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
- **í™•ì¥ì„± ë° ì‹ ë¢°ì„±:** ì§€ì—° ì—†ì´ ëŒ€ìš©ëŸ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.
- **ì°¨ë‹¨ ë° CAPTCHA ìš°íšŒ:** ê³ ê¸‰ ã‚¢ãƒ³ãƒãƒœãƒƒãƒˆ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
- **ê¸€ë¡œë²Œ IP ì»¤ë²„ë¦¬ì§€:** [Bright Data proxies](https://brightdata.co.kr/proxy-types)ë¥¼ í†µí•´ 195ê°œ êµ­ê°€ì—ì„œ ì ‘ê·¼í•©ë‹ˆë‹¤.
- **ì›í™œí•œ í†µí•©:** ì–´ë–¤ MCP clientì—ì„œë„ ë¹ ë¥´ê²Œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### Prerequisites for Bright Data MCP

Bright Data MCP í†µí•©ì„ ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒì„ í™•ì¸í•˜ì‹­ì‹œì˜¤:

1. **Bright Data Account:** [brightdata.com](https://brightdata.co.kr/)ì—ì„œ ë“±ë¡í•©ë‹ˆë‹¤. ì²« ì‚¬ìš©ìëŠ” í…ŒìŠ¤íŠ¸ìš© ë¬´ë£Œ í¬ë ˆë”§ì„ ë°›ìŠµë‹ˆë‹¤.
2. **API Token:** Bright Data ê³„ì • ì„¤ì •ì—ì„œ API tokenì„ í™•ë³´í•©ë‹ˆë‹¤([User Settings Page](https://brightdata.co.kr/cp/setting/users)).
3. **Web Unlocker Zone:** Bright Data ì œì–´ íŒ¨ë„ì—ì„œ [Web Unlocker proxy](https://docs.brightdata.com/scraping-automation/web-unlocker/quickstart) zoneì„ ìƒì„±í•©ë‹ˆë‹¤. `mcp_unlocker`ì²˜ëŸ¼ ê¸°ì–µí•˜ê¸° ì‰¬ìš´ ì‹ë³„ìë¥¼ ì„ íƒí•˜ì‹­ì‹œì˜¤(í•„ìš” ì‹œ í™˜ê²½ ë³€ìˆ˜ë¡œ ë‚˜ì¤‘ì— ë³€ê²½ ê°€ëŠ¥).
4. **(Optional) Scraping Browser Zone:** ê³ ê¸‰ ë¸Œë¼ìš°ì € ìë™í™” ê¸°ëŠ¥(ì˜ˆ: ë³µì¡í•œ JavaScript ìƒí˜¸ì‘ìš© ë˜ëŠ” ìŠ¤í¬ë¦°ìƒ·)ì´ í•„ìš”í•˜ë‹¤ë©´ [Scraping Browser zone](https://docs.brightdata.com/scraping-automation/scraping-browser/quickstart)ì„ ìƒì„±í•˜ì‹­ì‹œì˜¤. ì´ zoneì— ëŒ€í•´ ì œê³µë˜ëŠ” ì¸ì¦ ì •ë³´(Username ë° Password)ë¥¼(**Overview** íƒ­ì—ì„œ) ê¸°ë¡í•˜ì‹­ì‹œì˜¤. ì¼ë°˜ì ìœ¼ë¡œ `brd-customer-ACCOUNT_ID-zone-ZONE_NAME:PASSWORD` í˜•ì‹ì…ë‹ˆë‹¤.

### Quickstart: Configuring Bright Data MCP for Claude Desktop

**Step 1:** Bright Data MCP serverëŠ” ì¼ë°˜ì ìœ¼ë¡œ Node.jsì— ë²ˆë“¤ë¡œ í¬í•¨ëœ `npx`ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. í•„ìš”í•˜ë‹¤ë©´ [ê³µì‹ ì›¹ì‚¬ì´íŠ¸](https://nodejs.org/en/download)ì—ì„œ Node.jsë¥¼ ì„¤ì¹˜í•˜ì‹­ì‹œì˜¤.

**Step 2:** Claude Desktop -> `Settings` -> `Developer` -> `Edit Config` (`claude_desktop_config.json`)ë¥¼ ì—½ë‹ˆë‹¤.

**Step 3:** `mcpServers` ì•„ë˜ì— Bright Data server êµ¬ì„±ì„ ì‚½ì…í•©ë‹ˆë‹¤. í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ ìê²© ì¦ëª…ìœ¼ë¡œ ë°”ê¾¸ì‹­ì‹œì˜¤.

```json
{
  "mcpServers": {
    "Bright Data": { // Choose a name for the server
      "command": "npx",
      "args": ["@brightdata/mcp"],
      "env": {
        "API_TOKEN": "YOUR_BRIGHTDATA_API_TOKEN", // Paste your API token here
        "WEB_UNLOCKER_ZONE": "mcp_unlocker",     // Your Web Unlocker zone name
        // Optional: Add if using Scraping Browser tools
        "BROWSER_AUTH": "brd-customer-ACCOUNTID-zone-YOURZONE:PASSWORD"
      }
    }
  }
}
```

**Step 4:** êµ¬ì„± íŒŒì¼ì„ ì €ì¥í•œ ë’¤ Claude Desktopì„ ì¬ì‹œì‘í•©ë‹ˆë‹¤.

**Step 5:** Claude Desktopì—ì„œ ë§ì¹˜ ì•„ì´ì½˜(ğŸ”¨)ì— ë§ˆìš°ìŠ¤ë¥¼ ì˜¬ë¦¬ë©´, ì´ì œ ì—¬ëŸ¬ MCP toolì„ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

![claude-desktop-interface-with-mcp-tools-available](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/claude-desktop-interface-with-mcp-tools-available.png)

ã‚¹ã‚¯ãƒ¬ã‚¤í¼ë¥¼ ì œí•œí•  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì›¹ì‚¬ì´íŠ¸ë¡œ ì•Œë ¤ì§„ Zillowì—ì„œ ë°ì´í„° ì¶”ì¶œì„ ì‹œë„í•´ ë³´ê² ìŠµë‹ˆë‹¤. Claudeì— ë‹¤ìŒê³¼ ê°™ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì‹­ì‹œì˜¤: "_Extract key property data in JSON format from this Zillow URL: [https://www.zillow.com/apartments/arverne-ny/the-tides-at-arverne-by-the-sea/ChWHPZ/](https://www.zillow.com/apartments/arverne-ny/the-tides-at-arverne-by-the-sea/ChWHPZ/)_"

![bright-data-mcp-zillow-property-extraction-process](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/bright-data-mcp-zillow-property-extraction-process.png)

Claudeê°€ í•„ìš”í•œ Bright Data MCP toolsë¥¼ ì‚¬ìš©í•˜ë„ë¡ í—ˆìš©í•˜ì‹­ì‹œì˜¤. Bright Dataì˜ MCP serverê°€ ê¸°ë°˜ ë³µì¡ì„±(ãƒ—ãƒ­ã‚­ã‚· ë¡œí…Œì´ì…˜, í•„ìš” ì‹œ Scraping Browserë¥¼ í†µí•œ JavaScript ë Œë”ë§)ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

Bright Data serverê°€ ì¶”ì¶œì„ ìˆ˜í–‰í•˜ê³  êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ë©´, Claudeê°€ ì´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.

![zillow-property-data-json-structure-bright-data-mcp](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/zillow-property-data-json-structure-bright-data-mcp.png)

ê°€ëŠ¥í•œ ì¶œë ¥ì˜ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

```json
{
  "propertyInfo": {
    "name": "The Tides At Arverne By The Sea",
    "address": "190 Beach 69th St, Arverne, NY 11692",
    "propertyType": "Apartment building",
    // ... more info
  },
  "rentPrices": {
    "studio": { "startingPrice": "$2,750", /* ... */ },
    "oneBed": { "startingPrice": "$2,900", /* ... */ },
    "twoBed": { "startingPrice": "$3,350", /* ... */ }
  },
  // ... amenities, policies, etc.
}
```

**ë˜ ë‹¤ë¥¸ ì˜ˆ: Hacker News í—¤ë“œë¼ì¸**

ë” ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œëŠ” ë‹¤ìŒì´ ìˆìŠµë‹ˆë‹¤: "_Give me the titles of the latest 5 news articles from Hacker News_".

![hacker-news-latest-articles-mcp-extraction-results](https://github.com/luminati-io/web-scraping-with-mcp/blob/main/images/hacker-news-latest-articles-mcp-extraction-results.png)

ì´ëŠ” Bright Dataì˜ MCP serverê°€ AI ì›Œí¬í”Œë¡œ ë‚´ì—ì„œ ë™ì ì´ê±°ë‚˜ ê°•í•˜ê²Œ ë³´ì•ˆëœ ì›¹ ì½˜í…ì¸ ì—ë„ ì§ì ‘ ì ‘ê·¼í•˜ëŠ” ê³¼ì •ì„ ì–¼ë§ˆë‚˜ ë‹¨ìˆœí™”í•˜ëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.

## Further Reading

AI ë° ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì— ëŒ€í•´ ë” ê¹Šì´ ìˆëŠ” ì§€ì‹ì„ ì–»ê¸° ìœ„í•´, ì´ì „ ê°€ì´ë“œ ì¤‘ ì¼ë¶€ë¥¼ ì„ ë³„í•˜ì—¬ ì†Œê°œí•©ë‹ˆë‹¤:

- [Top Sources for Finding LLM Training Data](https://brightdata.co.kr/blog/web-data/llm-training-data)
- [Web Scraping with LLaMA 3: Turn Any Website into Structured JSON](https://brightdata.co.kr/blog/web-data/web-scraping-with-llama-3)
- [Web Scraping With LangChain and Bright Data](https://brightdata.co.kr/blog/web-data/web-scraping-with-langchain-and-bright-data)
- [How To Create a RAG Chatbot With GPT-4o Using SERP Data](https://brightdata.co.kr/blog/web-data/build-a-rag-chatbot)

## Conclusion

Anthropicì˜ Model Context Protocolì€ AI ì‹œìŠ¤í…œì´ ì™¸ë¶€ ì„¸ê³„ì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë°©ì‹ì— ê·¼ë³¸ì ì¸ ë³€í™”ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. íŠ¹ì • ì‘ì—…ì„ ìœ„í•´ ì»¤ìŠ¤í…€ MCP serverë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Bright Dataì˜ MCP í†µí•©ì€ ã‚¢ãƒ³ãƒãƒœãƒƒãƒˆ ë³´í˜¸ë¥¼ íšŒí”¼í•˜ê³  [AI-ready êµ¬ì¡°í™” ë°ì´í„°](https://brightdata.co.kr/use-cases/data-for-ai)ë¥¼ ì œê³µí•˜ëŠ” ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° ê¸°ëŠ¥ì„ ì „ë‹¬í•¨ìœ¼ë¡œì¨ ì´ë¥¼ í•œì¸µ ë” ê°•í™”í•©ë‹ˆë‹¤.

ì§€ê¸ˆ [AI solutions](https://brightdata.co.kr/ai)ì— ë“±ë¡í•˜ê³  ë¬´ë£Œë¡œ ì‚¬ìš©í•´ ë³´ì‹­ì‹œì˜¤!